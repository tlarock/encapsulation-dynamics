{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ff3cfe-f379-4a00-abd8-c9a3194c59a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "import xgi\n",
    "from utils import read_hyperedges, read_data, largest_connected_component\n",
    "from layer_randomization import layer_randomization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa324df-8d96-433a-8556-a3f09f6576f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prefix = \"../data/\"\n",
    "dataset_name = \"coauth-MAG-History\"\n",
    "dataset_path = f\"{data_prefix}{dataset_name}/\"\n",
    "\n",
    "observed_filename = dataset_path + f\"{dataset_name}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9f23c0-ed28-4970-b448-32a9b8906625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read observed data, compute largest CC, and construct hypergraph\n",
    "observed_hyperedges = read_data(dataset_path + dataset_name + \"-\", multiedges=False)\n",
    "#observed_hyperedges = read_hyperedges(observed_filename)\n",
    "observed_cc = largest_connected_component(observed_hyperedges, remove_single_nodes=False)\n",
    "observed_hg = xgi.Hypergraph(incoming_data=observed_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9961ba-797b-40b6-ab63-92d012d11f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute layer randomization, compute largest CC, and construct hypergraph\n",
    "layer_rnd_hyperedges = layer_randomization(observed_cc)\n",
    "layer_rnd_cc = largest_connected_component(layer_rnd_hyperedges, remove_single_nodes=False)\n",
    "layer_rnd_hg = xgi.Hypergraph(incoming_data=layer_rnd_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79863cb-4c1e-4124-b6e3-b9e2e2c9f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"observed: \", len(observed_hyperedges), len(observed_cc))\n",
    "print(\"layer randomization: \", len(layer_rnd_hyperedges), len(layer_rnd_cc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6d8d1a-d41a-4dbd-9380-f475f819c014",
   "metadata": {},
   "source": [
    "# Hyperedges Size Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11430c-8200-4d92-bf07-abb25c8132e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(6, 4), squeeze=False, sharey=True)\n",
    "row = col = 0\n",
    "\n",
    "print(len(observed_hg.edges.order.aslist()))\n",
    "print(sum(observed_hg.edges.order.aslist()))\n",
    "axs[row][col].hist(observed_hg.edges.order.aslist())\n",
    "axs[row][col].set(\n",
    "    title=\"Observed\",\n",
    "    yscale='log'\n",
    ")\n",
    "\n",
    "col = 1\n",
    "print(len(layer_rnd_hg.edges.order.aslist()))\n",
    "print(sum(layer_rnd_hg.edges.order.aslist()))\n",
    "axs[row][col].hist(layer_rnd_hg.edges.order.aslist())\n",
    "axs[row][col].set(\n",
    "    title=\"Layer\",\n",
    "    yscale='log'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115f4f39-9616-4033-99ba-70a8b1a7e105",
   "metadata": {},
   "source": [
    "# Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d435e-b8b7-4f2b-88ad-f9ce2ac23e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8, 4), squeeze=False, sharex=True, sharey=True)\n",
    "row = col = 0\n",
    "\n",
    "axs[row][col].hist(observed_hg.nodes.degree.aslist())\n",
    "axs[row][col].set(\n",
    "    title=\"Observed\",\n",
    "    yscale='log'\n",
    ")\n",
    "\n",
    "col = 1\n",
    "axs[row][col].hist(layer_rnd_hg.nodes.degree.aslist())\n",
    "axs[row][col].set(\n",
    "    title=\"Layer\",\n",
    "    yscale='log'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eade1c-30bf-4153-914f-9fca8d368211",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_deg_node = [node for node in observed_hg.nodes.filterby(\"degree\", observed_hg.nodes.degree.max())][0]\n",
    "print(max_deg_node)\n",
    "print(observed_hg.nodes.degree.max())\n",
    "deg_vect = [observed_hg.nodes.degree(order=k)[max_deg_node] for k in range(10)]\n",
    "print(deg_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba88f91-7fd0-4ad3-9f62-dbfff66419fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_deg_node = [node for node in layer_rnd_hg.nodes.filterby(\"degree\", layer_rnd_hg.nodes.degree.max())][0]\n",
    "print(max_deg_node)\n",
    "print(layer_rnd_hg.nodes.degree.max())\n",
    "deg_vect = [layer_rnd_hg.nodes.degree(order=k)[max_deg_node] for k in range(10)]\n",
    "print(deg_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dbc6ff-5db1-4050-b2a9-bc541d62e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, 11):\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8, 4), squeeze=False, sharex=True, sharey=True)\n",
    "    row = col = 0\n",
    "    fig.suptitle(k)\n",
    "    deg = observed_hg.nodes.degree(order=k).asnumpy()\n",
    "    axs[row][col].hist(deg[deg>0])\n",
    "    axs[row][col].set(\n",
    "        title=\"Observed\",\n",
    "        yscale='log'\n",
    "    )\n",
    "\n",
    "    col = 1\n",
    "    deg = layer_rnd_hg.nodes.degree(order=k).asnumpy()\n",
    "    axs[row][col].hist(deg[deg>0])\n",
    "    axs[row][col].set(\n",
    "        title=\"Layer\",\n",
    "        yscale='log'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d560493-3d05-4070-8ff9-ff42ecfc3bfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Overlap DAG (cross-layer overlap relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be34400-4411-42fc-8ae6-cca501417100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encapsulation_dag import overlap_dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09299c0-76c8-45c2-933c-24fd3bb3c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8, 4), squeeze=False, sharex=True)\n",
    "row = col = 0\n",
    "\n",
    "observed_ovdag, _, _ = overlap_dag(observed_cc)\n",
    "observed_outdist = [len(observed_ovdag[he]) for he in observed_ovdag]\n",
    "print(sum(observed_outdist))\n",
    "axs[row][col].hist(observed_outdist)\n",
    "axs[row][col].set(\n",
    "    title=\"Observed\",\n",
    "    yscale='log'\n",
    ")\n",
    "\n",
    "col = 1\n",
    "layer_rnd_overdag, _, _ = overlap_dag(layer_rnd_cc)\n",
    "layer_rnd_outdist = [len(layer_rnd_overdag[he]) for he in layer_rnd_overdag]\n",
    "print(sum(layer_rnd_outdist))\n",
    "axs[row][col].hist(layer_rnd_outdist)\n",
    "axs[row][col].set(\n",
    "    title=\"Layer\",\n",
    "    yscale='log'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57278f4e-1966-479c-adaa-a1974923a215",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Encapsulation DAG (cross-layer subset relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e08866b-296d-48b1-b030-9897f90deaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encapsulation_dag import encapsulation_dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf4fc60-b573-4a10-add4-65a5b5a33a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8, 4), squeeze=False, sharex=True)\n",
    "row = col = 0\n",
    "\n",
    "observed_ovdag, _, _ = encapsulation_dag(observed_cc)\n",
    "observed_outdist = [len(observed_ovdag[he]) for he in observed_ovdag]\n",
    "print(sum(observed_outdist))\n",
    "axs[row][col].hist(observed_outdist)\n",
    "axs[row][col].set(\n",
    "    title=\"Observed\",\n",
    "    yscale='log'\n",
    ")\n",
    "\n",
    "col = 1\n",
    "layer_rnd_overdag, _, _ = encapsulation_dag(layer_rnd_cc)\n",
    "layer_rnd_outdist = [len(layer_rnd_overdag[he]) for he in layer_rnd_overdag]\n",
    "print(sum(layer_rnd_outdist))\n",
    "axs[row][col].hist(layer_rnd_outdist)\n",
    "axs[row][col].set(\n",
    "    title=\"Layer\",\n",
    "    yscale='log'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23058400-d0b6-4496-92ec-149cb45c0855",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a724692-3fef-4f6a-8a70-e47e62702015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from encapsulation_dag import encapsulation_dag, overlap_dag, overlap_graph\n",
    "from utils import read_data, read_hyperedges, largest_connected_component\n",
    "from layer_randomization import layer_randomization\n",
    "\n",
    "def print_observed_stats(cc):\n",
    "    print(\"Computing encapsulation DAG...\")\n",
    "    obs_encap, _, _ = encapsulation_dag(cc)\n",
    "    print(\"Number of encapsulation DAG edges in observed data: \" + str(obs_encap.number_of_edges()))\n",
    "\n",
    "    print(\"Computing overlap DAG...\")\n",
    "    obs_overlap_dag, _, _ = overlap_dag(cc)\n",
    "    print(\"Number of overlap DAG in observed data: \" + str(obs_overlap_dag.number_of_edges()))\n",
    "\n",
    "    print(\"Computing overlap graph...\")\n",
    "    obs_overlap, _, _ = overlap_graph(cc, normalize_weight=False)\n",
    "    sum_of_weights = sum([data[\"weight\"] for _,_, data in obs_overlap.edges(data=True)])\n",
    "    print(\"Number of overlap edges in observed data: \" + str(obs_overlap.number_of_edges()))\n",
    "    return obs_encap.number_of_edges(), obs_overlap_dag.number_of_edges(), obs_overlap.number_of_edges(), sum_of_weights\n",
    "\n",
    "\n",
    "def print_random_stats(cc, obs_encap, obs_overdag, obs_overlap):\n",
    "    print(\"Computing encapsulation DAG...\")\n",
    "    rnd_encap, _, _ = encapsulation_dag(cc)\n",
    "    print(f\"Number of encapsulation DAG edges in random data: {rnd_encap.number_of_edges()} %: {rnd_encap.number_of_edges() / obs_encap}\")\n",
    "\n",
    "    print(\"Computing overlap DAG...\")\n",
    "    rnd_overlap_dag, _, _ = overlap_dag(cc)\n",
    "    print(f\"Number of overlap DAG edges in random data: {rnd_overlap_dag.number_of_edges()} %: {rnd_overlap_dag.number_of_edges() / obs_overdag}\")\n",
    "\n",
    "    print(\"Computing overlap graph...\")\n",
    "    rnd_overlap, _, _ = overlap_graph(cc, normalize_weight=False)\n",
    "    sum_of_weights = sum([data[\"weight\"] for _,_, data in rnd_overlap.edges(data=True)])\n",
    "    print(f\"Number of overlap edges in random data: {rnd_overlap.number_of_edges()} %: {rnd_overlap.number_of_edges() / obs_overlap}\")\n",
    "    return rnd_encap.number_of_edges(), rnd_overlap_dag.number_of_edges(), rnd_overlap.number_of_edges(), sum_of_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763d758b-09be-4586-bce1-64f4b7991f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8a14bf6-8c13-4ba7-9062-72b727f07cf4",
   "metadata": {},
   "source": [
    "# Compute/read changes in DAG stats after randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eb8430-4a36-47cb-9654-968c8d143437",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [#\"coauth-DBLP\",\n",
    "            \"email-Eu\",\n",
    "            \"coauth-MAG-Geology\",\n",
    "            \"coauth-MAG-History\",\n",
    "            \"contact-high-school\",\n",
    "            \"contact-primary-school\",\n",
    "            \"email-Enron\",\n",
    "\n",
    "]\n",
    "num_samples = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc3dc5f-e525-4972-b3f5-8cde9c549889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d424cb7-dc18-49a8-84d7-1fb9bbd75111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca11ee-fbaa-41fe-81d3-73e764b1f38a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "read_from_files = False\n",
    "if read_from_files:\n",
    "    obs_data = dict()\n",
    "    layer_data = dict()\n",
    "    for dataset_name in datasets:\n",
    "        with open(f\"../results/{dataset_name}/randomization_comparison.pickle\", \"rb\") as fpickle:\n",
    "            obs, layer = pickle.load(fpickle)\n",
    "            \n",
    "        obs_data[dataset_name] = obs\n",
    "        layer_data[dataset_name] = layer\n",
    "else:\n",
    "    for dataset_name in datasets:\n",
    "        print(dataset_name)\n",
    "        filename = f\"../data/{dataset_name}/{dataset_name}-\"\n",
    "\n",
    "        print(\"Reading hyperedges...\")\n",
    "        obs_hyperedges = read_data(filename, multiedges=False)\n",
    "        print(\"Done.\")\n",
    "\n",
    "        print(\"Computing largest connected component...\")\n",
    "        obs_cc = largest_connected_component(obs_hyperedges, remove_single_nodes=False)\n",
    "        print(\"Done.\")\n",
    "\n",
    "        obs_encap, obs_overdag, obs_overlap, obs_overlap_sum = print_observed_stats(obs_cc)\n",
    "        obs_data[dataset_name] = { \n",
    "            \"encap\": obs_encap,\n",
    "            \"overdag\": obs_overdag,\n",
    "            \"overlap\": obs_overlap,\n",
    "            \"overlap_sum\": obs_overlap_sum\n",
    "        }\n",
    "\n",
    "        layer_data[dataset_name] = {\n",
    "            \"encap\":[],\n",
    "            \"overdag\":[],\n",
    "            \"overlap\":[],\n",
    "            \"overlap_sum\":[]\n",
    "        }\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            print(f\"Computing layer randomization {i}...\")\n",
    "            random = layer_randomization(obs_hyperedges)\n",
    "            cc = largest_connected_component(random, remove_single_nodes=True)\n",
    "            encap, overdag, overlap, overlap_sum = print_random_stats(cc, obs_encap, obs_overdag, obs_overlap)\n",
    "            layer_data[dataset_name][\"encap\"].append(encap)\n",
    "            layer_data[dataset_name][\"overdag\"].append(overdag)\n",
    "            layer_data[dataset_name][\"overlap\"].append(overlap)\n",
    "            layer_data[dataset_name][\"overlap_sum\"].append(overlap_sum)\n",
    "            print()\n",
    "\n",
    "        Path(f\"../results/{dataset_name}/\").mkdir(parents=True, exist_ok=True)\n",
    "        with open(f\"../results/{dataset_name}/randomization_comparison.pickle\") as fpickle:\n",
    "            pickle.dump((obs_data, layer_data), fpickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9291598-d740-4579-8e92-635b85b413b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, len(datasets), squeeze=False, figsize=(35, 4), sharey=True)\n",
    "for col, dataset_name in enumerate(datasets):\n",
    "    obs_bars = []\n",
    "    layer_bars = []\n",
    "    layer_errs = []\n",
    "    for key in [\"encap\", \"overlap\", \"overlap_sum\"]:\n",
    "        #obs_bars.append()\n",
    "        layer = np.array(layer_data[dataset_name][key]) / obs_data[dataset_name][key]\n",
    "        layer_bars.append(np.mean(layer))\n",
    "        layer_errs.append(np.std(layer))\n",
    "\n",
    "    x = np.array(list(range(1, len(layer_bars)+1)))\n",
    "    width=0.1\n",
    "    #axs[0][col].bar(x-width, obs_bars, width=width, label=\"Observed\")\n",
    "    axs[0][col].bar(x, layer_bars, yerr=layer_errs, width=width, label=\"Layer Randomization\")\n",
    "    axs[0][col].set_xticks(x, labels=[f\"Encap. Edges\\n{obs_data[dataset_name]['encap']}\",\n",
    "                                      f\"Overlap Edges\\n{obs_data[dataset_name]['overlap']}\",\n",
    "                                      f\"Total Overlap\\n{obs_data[dataset_name]['overlap_sum']}\"], size=14)\n",
    "    axs[0][col].set_yticks([0.25, 0.5, 0.75, 1.0], labels=[\"0.25\", \"0.50\", \"0.75\", \"1.00\"], size=20)\n",
    "    axs[0][col].set_title(dataset_name, size=21)\n",
    "    axs[0][col].set_ylim((0, 1.1))\n",
    "    axs[0][col].set_xlim((0.5,len(layer_bars)+0.25))\n",
    "    axs[0][col].spines['top'].set_visible(False)\n",
    "    axs[0][col].spines['right'].set_visible(False)\n",
    "#fig.savefig(\"../results/plots/layer_randomization_comparison.pdf\", bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543ce50d-bcc0-4fa4-978b-e93bbfff0407",
   "metadata": {},
   "source": [
    "# Read component size information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2896dcf-2aa7-491d-a712-e153f81ee0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def read_component_data(dataset, num_samples, data_dir=\"../data/\"):\n",
    "    dataset_info = dict()\n",
    "    # Compute observed DAG\n",
    "    observed_path = data_dir + dataset + \"/\" + dataset + \"-\" \n",
    "    # Read heights by node dict\n",
    "    with open(data_dir + dataset + \"/\" + dataset + \"_dag_components.txt\", \"r\") as fin:\n",
    "        obs_components = np.array(list(map(int, fin.readline().split(','))))\n",
    "    dataset_info[\"observed_components_dist\"] = obs_components\n",
    "    \n",
    "    # Get a random DAG\n",
    "    with open(data_dir + dataset + \"/\" + dataset + \"_layer_randomization_dag_components.txt\", 'r') as fin:\n",
    "        random_comps = []\n",
    "        for line in fin:\n",
    "            random_comps.append(np.array(list(map(int, line.split(',')))))\n",
    "    dataset_info[\"random_comps\"] = random_comps\n",
    "    \n",
    "    # Get averages of random count distributions\n",
    "    random_count_dists = dict()\n",
    "    for arr in random_comps:\n",
    "        arr_counts = dict(Counter(arr))\n",
    "        for key in arr_counts:\n",
    "            if key in random_count_dists:\n",
    "                random_count_dists[key].append(arr_counts[key])\n",
    "            else:\n",
    "                random_count_dists[key] = [arr_counts[key]]\n",
    "\n",
    "    dataset_info[\"random_count_dists\"] = random_count_dists\n",
    "    \n",
    "    random_means = dict()\n",
    "    #random_stds = dict()\n",
    "    for key in random_count_dists:\n",
    "        random_means[key] = sum(random_count_dists[key]) / num_samples\n",
    "        #random_stds[key] = np.std(random_count_dists[key])\n",
    "\n",
    "    # Fill in missing values from both counters\n",
    "    observed_counts = dict(Counter(obs_components))\n",
    "    for c in set(observed_counts.keys()).union(set(random_means.keys())):\n",
    "        if c not in random_means:\n",
    "            random_means[c] = 0\n",
    "            #random_stds[c] = 0\n",
    "\n",
    "        if c not in observed_counts:\n",
    "            observed_counts[c] = 0\n",
    "\n",
    "    dataset_info[\"observed_counts\"] = observed_counts\n",
    "    dataset_info[\"random_means\"] = random_means\n",
    "    #dataset_info[\"random_stds\"] = random_stds\n",
    "    return dataset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc320e0-89ae-4668-9dae-76cff276ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binning(values, num_bins = 15, log_binning = False, is_pmf = True, bins=None):   \n",
    "    if bins is None:\n",
    "        # We need to define the support of our distribution\n",
    "        lower_bound = min(values)\n",
    "        upper_bound = max(values)\n",
    "\n",
    "        # And the type of binning we want\n",
    "        if log_binning:\n",
    "            lower_bound = np.log10(lower_bound)\n",
    "            upper_bound = np.log10(upper_bound)+1\n",
    "            bins = np.logspace(lower_bound,upper_bound,num_bins+1, base = 10)\n",
    "        else:\n",
    "            bins = np.linspace(lower_bound,upper_bound,num_bins+1)\n",
    "    \n",
    "    # Then we can compute the histogram using numpy\n",
    "    if is_pmf:\n",
    "        y, __ = np.histogram(values, bins = bins, density=False)\n",
    "        p = y/float(y.sum())\n",
    "        \n",
    "    else:\n",
    "        p, __ = np.histogram(values, bins = bins, density=False)\n",
    "    \n",
    "    # Now, we need to compute for each y the value of x\n",
    "    x = bins[1:] - np.diff(bins)/2.0    \n",
    "    \n",
    "    if bins is None:\n",
    "        x = x[p>0]\n",
    "        p = p[p>0]\n",
    "\n",
    "    return x, p, bins\n",
    "\n",
    "def bin_distributions(dataset_info, log_binning=True, num_bins=50, is_pmf=True):\n",
    "    # Bin the observed distribution\n",
    "    obs_comps_dist = dataset_info[\"observed_components_dist\"]\n",
    "    x, y, bins = get_binning(obs_comps_dist, num_bins = num_bins, log_binning = log_binning, is_pmf = is_pmf)\n",
    "    dataset_info[\"obs_x\"] = x\n",
    "    dataset_info[\"obs_y\"] = y\n",
    "    \n",
    "    # Bin the random distribution\n",
    "    rnd_comps_dists = dataset_info[\"random_comps\"]\n",
    "    #rnd_array = np.zeros(y.shape[0])\n",
    "    #for dist in rnd_comps_dists:\n",
    "    #    x, y, bins = get_binning(dist, log_binning = log_binning, is_pmf = True, bins=bins)\n",
    "    #    rnd_array += y\n",
    "    #rnd_array /= len(dist)\n",
    "    #x, rnd_array, bins = get_binning(rnd_comps_dists[0], log_binning = log_binning, is_pmf = True, bins=bins)\n",
    "    \n",
    "    rnd_lists = [[] for _ in x]\n",
    "    for dist in rnd_comps_dists:\n",
    "        x, y, bins = get_binning(dist, log_binning = log_binning, is_pmf = is_pmf, bins=bins)\n",
    "        for idx, val in enumerate(y):\n",
    "            rnd_lists[idx].append(val)\n",
    "    \n",
    "    rnd_array = np.zeros(y.shape[0])\n",
    "    for idx in range(len(x)):\n",
    "        rnd_array[idx] = np.median(rnd_lists[idx])\n",
    "    #rnd_array /= len(dist)\n",
    "    #x, rnd_array, bins = get_binning(rnd_comps_dists[0], log_binning = log_binning, is_pmf = True, bins=bins)\n",
    "    dataset_info[\"rnd_x\"] = x\n",
    "    dataset_info[\"rnd_y\"] = rnd_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0e646a-1375-4582-9b96-5f020f18bdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"coauth-MAG-Geology\", \"coauth-MAG-History\",  \"contact-high-school\", \"contact-primary-school\", \"email-Enron\", \"email-Eu\"]\n",
    "dataset_info_dicts = dict()\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    dataset_info_dicts[dataset] = read_component_data(dataset, num_samples)\n",
    "    bin_distributions(dataset_info_dicts[dataset], is_pmf = False, log_binning=True, num_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00056ee3-3eff-4a31-b5e2-507947cb4a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, len(datasets), squeeze=False, figsize=(35, 4))\n",
    "for col, dataset_name in enumerate(datasets):\n",
    "    observed_x = dataset_info_dicts[dataset_name][\"obs_x\"]\n",
    "    observed_y = dataset_info_dicts[dataset_name][\"obs_y\"]\n",
    "    axs[0][col].scatter(observed_x, observed_y, label=\"Observed\")\n",
    "    \n",
    "    random_x = dataset_info_dicts[dataset_name][\"rnd_x\"]\n",
    "    random_y = dataset_info_dicts[dataset_name][\"rnd_y\"]\n",
    "    axs[0][col].scatter(random_x, random_y, label=\"Random\", alpha=0.8, marker='^')\n",
    "    #axs[0][col].set_title(dataset_name, size=21)\n",
    "    axs[0][col].set(yscale='log', xscale='log', xlabel=\"Component Size\")\n",
    "    if col == 0:\n",
    "        axs[0][col].set_ylabel(\"Number of Components\")\n",
    "        axs[0][col].legend(frameon=False, fontsize=16)\n",
    "    axs[0][col].spines['top'].set_visible(False)\n",
    "    axs[0][col].spines['right'].set_visible(False)\n",
    "\n",
    "#fig.savefig(\"../results/plots/components.pdf\", bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eed58b8-7f23-4b11-8de3-762b0a4f02be",
   "metadata": {},
   "source": [
    "# Plot both together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59f6e8a-cf7b-44d6-9287-16d1f06b01e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, len(datasets), squeeze=False, figsize=(35, 8), sharey=False)\n",
    "plt.rcParams[\"xtick.labelsize\"] = 13\n",
    "plt.rcParams[\"ytick.labelsize\"] = 14\n",
    "plt.rcParams['axes.labelsize'] = 20\n",
    "row_idx = 0\n",
    "for col, dataset_name in enumerate(datasets):\n",
    "    obs_bars = []\n",
    "    layer_bars = []\n",
    "    layer_errs = []\n",
    "    for key in [\"encap\", \"overlap\", \"overlap_sum\"]:\n",
    "        #obs_bars.append()\n",
    "        layer = np.array(layer_data[dataset_name][key]) / obs_data[dataset_name][key]\n",
    "        layer_bars.append(np.mean(layer))\n",
    "        layer_errs.append(np.std(layer))\n",
    "\n",
    "    x = np.array(list(range(1, len(layer_bars)+1)))\n",
    "    width=0.1\n",
    "    axs[row_idx][col].bar(x, layer_bars, yerr=layer_errs, width=width, label=\"Layer Randomization\")\n",
    "    axs[row_idx][col].set_xticks(x, labels=[f\"Encap. Edges\\n{obs_data[dataset_name]['encap']}\",\n",
    "                                      f\"Overlap Edges\\n{obs_data[dataset_name]['overlap']}\",\n",
    "                                      f\"Total Overlap\\n{obs_data[dataset_name]['overlap_sum']}\"])\n",
    "    axs[row_idx][col].set_yticks([0.25, 0.5, 0.75, 1.0], labels=[\"0.25\", \"0.50\", \"0.75\", \"1.00\"])\n",
    "    axs[row_idx][col].set_title(dataset_name, size=21)\n",
    "    axs[row_idx][col].set_ylim((0, 1.1))\n",
    "    axs[row_idx][col].set_xlim((0.5,len(layer_bars)+0.25))\n",
    "    axs[row_idx][col].spines['top'].set_visible(False)\n",
    "    axs[row_idx][col].spines['right'].set_visible(False)\n",
    "    if col == 0:\n",
    "        axs[row_idx][col].set(ylabel=\"Fraction of Observed\")\n",
    "\n",
    "\n",
    "row_idx = 1\n",
    "for col, dataset_name in enumerate(datasets):\n",
    "    observed_x = dataset_info_dicts[dataset_name][\"obs_x\"]\n",
    "    observed_y = dataset_info_dicts[dataset_name][\"obs_y\"]\n",
    "    axs[row_idx][col].scatter(observed_x, observed_y, label=\"Observed\")\n",
    "    \n",
    "    random_x = dataset_info_dicts[dataset_name][\"rnd_x\"]\n",
    "    random_y = dataset_info_dicts[dataset_name][\"rnd_y\"]\n",
    "    axs[row_idx][col].scatter(random_x, random_y, label=\"Random\", alpha=0.8, marker='^')\n",
    "    axs[row_idx][col].set(yscale='log', xscale='log', xlabel=\"Component Size\")\n",
    "    if col == 0:\n",
    "        axs[row_idx][col].set_ylabel(\"# Components\")\n",
    "        axs[row_idx][col].legend(frameon=False, fontsize=16)\n",
    "    axs[row_idx][col].spines['top'].set_visible(False)\n",
    "    axs[row_idx][col].spines['right'].set_visible(False)\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "#fig.savefig(\"../results/plots/layer_randomization_comparison_combined.pdf\", bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403d13e4-6ae1-4483-95f4-48ee2b455896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1340d9-e53c-40a4-a95e-8eba1fc2b7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
