{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff8b95d-d919-40df-acec-48eb075d7c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from utils import read_data\n",
    "from encapsulation_dag import encapsulation_dag\n",
    "from layer_randomization import layer_randomization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import matplotlib_defaults\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edd48dd-e2ef-474a-b6c9-dc5b77c47d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/\"\n",
    "datasets = [\"email-Enron\", \"email-Eu\", \"contact-primary-school\", \"contact-high-school\", \"coauth-MAG-History\", \"coauth-MAG-Geology\"]#, \"coauth-DBLP\"]\n",
    "num_samples = 5\n",
    "remove_single_nodes = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da31866-736e-4572-890e-5ddced8bf61b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    observed_path = data_dir + dataset + \"/\" + dataset + \"-\" \n",
    "    print(\"Reading hyperedges.\")\n",
    "    hyperedges = read_data(observed_path, multiedges=False)\n",
    "    if remove_single_nodes:\n",
    "        hyperedges = [he for he in hyperedges if len(he) > 1]\n",
    "\n",
    "    print(\"Computing observed dag.\")\n",
    "    obs_dag, obs_nth, obs_he_map = encapsulation_dag(hyperedges)\n",
    "\n",
    "    print(f\"Dag edges: {obs_dag.number_of_edges()}\")\n",
    "\n",
    "    # Observed\n",
    "    dag = obs_dag\n",
    "    component_sizes = [len(c) for c in nx.weakly_connected_components(obs_dag)]\n",
    "    components_output_file = data_dir + dataset + f\"/{dataset}_dag_components.txt\"\n",
    "    with open(components_output_file, \"w\") as fout:\n",
    "        fout.write(\",\".join(map(str,component_sizes)))\n",
    "\n",
    "    # Random\n",
    "    random_comps = []\n",
    "    for _ in range(num_samples):\n",
    "        print(\"Computing layer randomization.\")\n",
    "        random_hyperedges = layer_randomization(hyperedges)\n",
    "        #### Heights ####\n",
    "        print(\"Computing random dag.\")\n",
    "        random_dag, _, _ = encapsulation_dag(random_hyperedges)\n",
    "        print(f\"Random dag has {random_dag.number_of_edges()} edges.\")\n",
    "        random_component_sizes = [len(c) for c in nx.weakly_connected_components(random_dag)]\n",
    "        random_comps.append(random_component_sizes)\n",
    "\n",
    "    components_output_file = data_dir + dataset + f\"/{dataset}_layer_randomization_dag_components.txt\"\n",
    "    with open(components_output_file, \"w\") as fout:\n",
    "        for sample_comp in random_comps:\n",
    "            fout.write(\",\".join(map(str,sample_comp)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4147e9c9-f3ab-47a8-b188-6ffc3d6d6182",
   "metadata": {},
   "source": [
    "# All datasets on one plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec00c02-1d98-4487-8500-f13b29e28d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_component_data(dataset, num_samples, data_dir=\"../data/\"):\n",
    "    dataset_info = dict()\n",
    "    # Compute observed DAG\n",
    "    observed_path = data_dir + dataset + \"/\" + dataset + \"-\" \n",
    "    # Read heights by node dict\n",
    "    with open(data_dir + dataset + \"/\" + dataset + \"_dag_components.txt\", \"r\") as fin:\n",
    "        obs_components = np.array(list(map(int, fin.readline().split(','))))\n",
    "    dataset_info[\"observed_components_dist\"] = obs_components\n",
    "    \n",
    "    # Get a random DAG\n",
    "    with open(data_dir + dataset + \"/\" + dataset + \"_layer_randomization_dag_components.txt\", 'r') as fin:\n",
    "        random_comps = []\n",
    "        for line in fin:\n",
    "            random_comps.append(np.array(list(map(int, line.split(',')))))\n",
    "    dataset_info[\"random_comps\"] = random_comps\n",
    "    \n",
    "    # Get averages of random count distributions\n",
    "    random_count_dists = dict()\n",
    "    for arr in random_comps:\n",
    "        arr_counts = dict(Counter(arr))\n",
    "        for key in arr_counts:\n",
    "            if key in random_count_dists:\n",
    "                random_count_dists[key].append(arr_counts[key])\n",
    "            else:\n",
    "                random_count_dists[key] = [arr_counts[key]]\n",
    "\n",
    "    dataset_info[\"random_count_dists\"] = random_count_dists\n",
    "    \n",
    "    random_means = dict()\n",
    "    #random_stds = dict()\n",
    "    for key in random_count_dists:\n",
    "        random_means[key] = sum(random_count_dists[key]) / num_samples\n",
    "        #random_stds[key] = np.std(random_count_dists[key])\n",
    "\n",
    "    # Fill in missing values from both counters\n",
    "    observed_counts = dict(Counter(obs_components))\n",
    "    for c in set(observed_counts.keys()).union(set(random_means.keys())):\n",
    "        if c not in random_means:\n",
    "            random_means[c] = 0\n",
    "            #random_stds[c] = 0\n",
    "\n",
    "        if c not in observed_counts:\n",
    "            observed_counts[c] = 0\n",
    "\n",
    "    dataset_info[\"observed_counts\"] = observed_counts\n",
    "    dataset_info[\"random_means\"] = random_means\n",
    "    #dataset_info[\"random_stds\"] = random_stds\n",
    "    return dataset_info\n",
    "\n",
    "datasets = [\"coauth-MAG-Geology\", \"coauth-MAG-History\",  \"contact-high-school\", \"contact-primary-school\", \"email-Enron\", \"email-Eu\"]\n",
    "dataset_info_dicts = dict()\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    dataset_info_dicts[dataset] = read_component_data(dataset, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420a6297-5d98-41ad-8f79-ba65c5c93cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binning(values, num_bins = 15, log_binning = False, is_pmf = True, bins=None):   \n",
    "    if bins is None:\n",
    "        # We need to define the support of our distribution\n",
    "        lower_bound = min(values)\n",
    "        upper_bound = max(values)\n",
    "\n",
    "        # And the type of binning we want\n",
    "        if log_binning:\n",
    "            lower_bound = np.log10(lower_bound)\n",
    "            upper_bound = np.log10(upper_bound)+1\n",
    "            bins = np.logspace(lower_bound,upper_bound,num_bins+1, base = 10)\n",
    "        else:\n",
    "            bins = np.linspace(lower_bound,upper_bound,num_bins+1)\n",
    "    \n",
    "    # Then we can compute the histogram using numpy\n",
    "    if is_pmf:\n",
    "        y, __ = np.histogram(values, bins = bins, density=False)\n",
    "        p = y/float(y.sum())\n",
    "        \n",
    "    else:\n",
    "        p, __ = np.histogram(values, bins = bins, density=False)\n",
    "    \n",
    "    # Now, we need to compute for each y the value of x\n",
    "    x = bins[1:] - np.diff(bins)/2.0    \n",
    "    \n",
    "    if bins is None:\n",
    "        x = x[p>0]\n",
    "        p = p[p>0]\n",
    "\n",
    "    return x, p, bins\n",
    "\n",
    "def bin_distributions(dataset_info, log_binning=True, num_bins=50, is_pmf=True):\n",
    "    # Bin the observed distribution\n",
    "    obs_comps_dist = dataset_info[\"observed_components_dist\"]\n",
    "    x, y, bins = get_binning(obs_comps_dist, num_bins = num_bins, log_binning = log_binning, is_pmf = is_pmf)\n",
    "    dataset_info[\"obs_x\"] = x\n",
    "    dataset_info[\"obs_y\"] = y\n",
    "    \n",
    "    # Bin the random distribution\n",
    "    rnd_comps_dists = dataset_info[\"random_comps\"]\n",
    "    #rnd_array = np.zeros(y.shape[0])\n",
    "    #for dist in rnd_comps_dists:\n",
    "    #    x, y, bins = get_binning(dist, log_binning = log_binning, is_pmf = True, bins=bins)\n",
    "    #    rnd_array += y\n",
    "    #rnd_array /= len(dist)\n",
    "    #x, rnd_array, bins = get_binning(rnd_comps_dists[0], log_binning = log_binning, is_pmf = True, bins=bins)\n",
    "    \n",
    "    rnd_lists = [[] for _ in x]\n",
    "    for dist in rnd_comps_dists:\n",
    "        x, y, bins = get_binning(dist, log_binning = log_binning, is_pmf = is_pmf, bins=bins)\n",
    "        for idx, val in enumerate(y):\n",
    "            rnd_lists[idx].append(val)\n",
    "    \n",
    "    rnd_array = np.zeros(y.shape[0])\n",
    "    for idx in range(len(x)):\n",
    "        rnd_array[idx] = np.median(rnd_lists[idx])\n",
    "    #rnd_array /= len(dist)\n",
    "    #x, rnd_array, bins = get_binning(rnd_comps_dists[0], log_binning = log_binning, is_pmf = True, bins=bins)\n",
    "    dataset_info[\"rnd_x\"] = x\n",
    "    dataset_info[\"rnd_y\"] = rnd_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2096b652-ccec-4082-8c8a-fa89920724ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in dataset_info_dicts:\n",
    "    bin_distributions(dataset_info_dicts[dataset_name], is_pmf = False, log_binning=True, num_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d13af92-522a-4f41-bb15-b90e2de1bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, len(datasets), squeeze=False, figsize=(35, 4))\n",
    "for col, dataset_name in enumerate(datasets):\n",
    "    observed_x = dataset_info_dicts[dataset_name][\"obs_x\"]\n",
    "    observed_y = dataset_info_dicts[dataset_name][\"obs_y\"]\n",
    "    axs[0][col].scatter(observed_x, observed_y, label=\"Observed\")\n",
    "    \n",
    "    random_x = dataset_info_dicts[dataset_name][\"rnd_x\"]\n",
    "    random_y = dataset_info_dicts[dataset_name][\"rnd_y\"]\n",
    "    axs[0][col].scatter(random_x, random_y, label=\"Random\", alpha=0.8, marker='^')\n",
    "    #axs[0][col].set_title(dataset_name, size=21)\n",
    "    axs[0][col].set(yscale='log', xscale='log', xlabel=\"Component Size\")\n",
    "    if col == 0:\n",
    "        axs[0][col].set_ylabel(\"Number of Components\")\n",
    "        axs[0][col].legend(frameon=False, fontsize=16)\n",
    "    axs[0][col].spines['top'].set_visible(False)\n",
    "    axs[0][col].spines['right'].set_visible(False)\n",
    "\n",
    "#fig.savefig(\"../results/plots/components.pdf\", bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aba7a1-c1ca-4952-a123-dba7c275a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, len(datasets), squeeze=False, figsize=(35, 4))\n",
    "for col, dataset_name in enumerate(datasets):\n",
    "    obs_counts = dataset_info_dicts[dataset_name][\"observed_counts\"]\n",
    "    x = sorted(list(obs_counts.keys()))\n",
    "    y = [obs_counts[xval] for xval in x]\n",
    "    axs[0][col].scatter(x, y, label=\"Observed\")\n",
    "    \n",
    "    random_means = dataset_info_dicts[dataset_name][\"random_means\"]\n",
    "    x = sorted(list(random_means.keys()))\n",
    "    y = [random_means[xval] for xval in x]\n",
    "    axs[0][col].scatter(x, y, label=\"Random\", alpha=0.3)\n",
    "    \n",
    "    if col == 0:\n",
    "        axs[0][col].legend()\n",
    "\n",
    "    axs[0][col].set_title(dataset_name, size=21)\n",
    "    axs[0][col].set(yscale='log', xscale='log', xlabel=\"Component Size\", ylabel=\"Count\")\n",
    "    axs[0][col].spines['top'].set_visible(False)\n",
    "    axs[0][col].spines['right'].set_visible(False)\n",
    "\n",
    "#fig.savefig(\"../results/plots/layer_randomization_comparison.pdf\", bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ed9f05-1266-46d7-b1ff-3e6d3878276f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
